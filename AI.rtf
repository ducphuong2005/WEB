{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww14740\viewh8980\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 BFS:\
from collections import deque\
\
def bfs(graph, start):\
    visited = set()\
    queue = deque([start])\
    visited.add(start)\
\
    print("BFS traversal:")\
    while queue:\
        node = queue.popleft()\
        print(node, end=" ")\
\
        for neighbor in graph[node]:\
            if neighbor not in visited:\
                visited.add(neighbor)\
                queue.append(neighbor)\
\
# V\'ed d\uc0\u7909 \
graph = \{\
    'A': ['B', 'C'],\
    'B': ['D', 'E'],\
    'C': ['F'],\
    'D': [],\
    'E': [],\
    'F': []\
\}\
\
bfs(graph, 'A')\
\
\
\
DFS:\
def dfs(graph, node, visited=None):\
    if visited is None:\
        visited = set()\
\
    visited.add(node)\
    print(node, end=" ")\
\
    for neighbor in graph[node]:\
        if neighbor not in visited:\
            dfs(graph, neighbor, visited)\
\
# V\'ed d\uc0\u7909 \
print("DFS traversal:")\
dfs(graph, 'A')\
\
\
\
\
IDDFS:\
def dls(graph, node, depth, visited):\
    if depth == 0:\
        print(node, end=" ")\
        return\
\
    for neighbor in graph[node]:\
        if neighbor not in visited:\
            visited.add(neighbor)\
            dls(graph, neighbor, depth - 1, visited)\
\
def iddfs(graph, start, max_depth):\
    for depth in range(max_depth + 1):\
        print(f"\\nDepth \{depth\}: ", end="")\
        dls(graph, start, depth, set([start]))\
\
# V\'ed d\uc0\u7909 \
iddfs(graph, 'A', 3)\
\
\
A*:\
import heapq\
\
def a_star(graph, start, goal, heuristic):\
    open_list = []\
    heapq.heappush(open_list, (0, start))\
    came_from = \{\}\
    g_cost = \{start: 0\}\
\
    while open_list:\
        _, current = heapq.heappop(open_list)\
\
        if current == goal:\
            path = []\
            while current in came_from:\
                path.append(current)\
                current = came_from[current]\
            path.append(start)\
            return path[::-1]\
\
        for neighbor, cost in graph[current]:\
            new_cost = g_cost[current] + cost\
            if neighbor not in g_cost or new_cost < g_cost[neighbor]:\
                g_cost[neighbor] = new_cost\
                f_cost = new_cost + heuristic(neighbor)\
                heapq.heappush(open_list, (f_cost, neighbor))\
                came_from[neighbor] = current\
\
    return None\
\
# V\'ed d\uc0\u7909 \
graph_a = \{\
    'A': [('B', 1), ('C', 3)],\
    'B': [('D', 1)],\
    'C': [('D', 1)],\
    'D': []\
\}\
\
h = lambda x: \{'A':3, 'B':2, 'C':1, 'D':0\}[x]\
\
print("A* path:", a_star(graph_a, 'A', 'D', h))\
\
\
\
\
Linear Regression \
import numpy as np\
\
def linear_regression(X, y, lr=0.01, epochs=1000):\
    m, b = 0.0, 0.0\
    n = len(X)\
\
    for _ in range(epochs):\
        y_pred = m * X + b\
        dm = (-2/n) * np.sum(X * (y - y_pred))\
        db = (-2/n) * np.sum(y - y_pred)\
        m -= lr * dm\
        b -= lr * db\
\
    return m, b\
\
# V\'ed d\uc0\u7909 \
X = np.array([1, 2, 3, 4, 5])\
y = np.array([2, 4, 6, 8, 10])\
\
m, b = linear_regression(X, y)\
print("y =", m, "* x +", b)\
\
\
\
\
Logistic Regression \
import numpy as np\
\
def sigmoid(z):\
    return 1 / (1 + np.exp(-z))\
\
def logistic_regression(X, y, lr=0.1, epochs=1000):\
    w = np.zeros(X.shape[1])\
    b = 0\
\
    for _ in range(epochs):\
        z = np.dot(X, w) + b\
        y_pred = sigmoid(z)\
        dw = np.dot(X.T, (y_pred - y)) / len(y)\
        db = np.sum(y_pred - y) / len(y)\
        w -= lr * dw\
        b -= lr * db\
\
    return w, b\
\
# V\'ed d\uc0\u7909 \
X = np.array([[1],[2],[3],[4]])\
y = np.array([0,0,1,1])\
\
w, b = logistic_regression(X, y)\
print("Weights:", w, "Bias:", b)\
\
\
\
Naive Bayes:\
import numpy as np\
\
class NaiveBayes:\
    def fit(self, X, y):\
        self.classes = np.unique(y)\
        self.mean = \{\}\
        self.var = \{\}\
        self.prior = \{\}\
\
        for c in self.classes:\
            X_c = X[y == c]\
            self.mean[c] = X_c.mean(axis=0)\
            self.var[c] = X_c.var(axis=0)\
            self.prior[c] = X_c.shape[0] / X.shape[0]\
\
    def predict(self, X):\
        predictions = []\
        for x in X:\
            posteriors = []\
            for c in self.classes:\
                prior = np.log(self.prior[c])\
                likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.var[c]))\
                likelihood -= np.sum((x - self.mean[c])**2 / (2 * self.var[c]))\
                posteriors.append(prior + likelihood)\
            predictions.append(self.classes[np.argmax(posteriors)])\
        return predictions\
\
# V\'ed d\uc0\u7909 \
X = np.array([[1,2],[2,3],[3,4],[6,5],[7,8],[8,9]])\
y = np.array([0,0,0,1,1,1])\
\
nb = NaiveBayes()\
nb.fit(X, y)\
print("Prediction:", nb.predict([[2,3],[7,8]]))\
\
\
\
\
ID3, Entropy, in4 gain:\
import numpy as np\
from collections import Counter\
import math\
\
def entropy(y):\
    counts = Counter(y)\
    total = len(y)\
    return -sum((c/total) * math.log2(c/total) for c in counts.values())\
\
def information_gain(X, y, feature_index):\
    total_entropy = entropy(y)\
    values = set(X[:, feature_index])\
\
    weighted_entropy = 0\
    for v in values:\
        y_subset = y[X[:, feature_index] == v]\
        weighted_entropy += (len(y_subset)/len(y)) * entropy(y_subset)\
\
    return total_entropy - weighted_entropy\
\
# V\'ed d\uc0\u7909 \
X = np.array([\
    ['Sunny','Hot'],\
    ['Sunny','Cool'],\
    ['Rainy','Cool'],\
    ['Rainy','Hot']\
])\
y = np.array(['No','Yes','Yes','No'])\
\
print("IG feature 0:", information_gain(X, y, 0))\
\
\
\
Decision tree:\
from sklearn.tree import DecisionTreeClassifier\
from sklearn.model_selection import train_test_split\
from sklearn.datasets import load_iris\
\
data = load_iris()\
X, y = data.data, data.target\
\
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\
\
model = DecisionTreeClassifier(criterion="entropy")\
model.fit(X_train, y_train)\
\
print("Accuracy:", model.score(X_test, y_test))\
\
\
\
Perceptron:\
class Perceptron:\
    def __init__(self, lr=0.1, epochs=10):\
        self.lr = lr\
        self.epochs = epochs\
\
    def fit(self, X, y):\
        self.w = [0] * len(X[0])\
        self.b = 0\
\
        for _ in range(self.epochs):\
            for xi, yi in zip(X, y):\
                y_pred = 1 if sum(w*x for w, x in zip(self.w, xi)) + self.b >= 0 else 0\
                error = yi - y_pred\
                self.w = [w + self.lr * error * x for w, x in zip(self.w, xi)]\
                self.b += self.lr * error\
\
# V\'ed d\uc0\u7909  (c\u7893 ng AND)\
X = [[0,0],[0,1],[1,0],[1,1]]\
y = [0,0,0,1]\
\
p = Perceptron()\
p.fit(X, y)\
print("Weights:", p.w, "Bias:", p.b)\
\
\
\
\
\pard\pardeftab720\partightenfactor0

\f1 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 MULTI-LAYER PERCEPTRON:\
from sklearn.neural_network import MLPClassifier\
from sklearn.datasets import load_iris\
from sklearn.model_selection import train_test_split\
\
data = load_iris()\
X, y = data.data, data.target\
\
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\
\
mlp = MLPClassifier(hidden_layer_sizes=(50,50),\
                    activation='relu',\
                    max_iter=1000)\
\
mlp.fit(X_train, y_train)\
print("MLP Accuracy:", mlp.score(X_test, y_test))\

\f0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
}